import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from collections import deque
import random
import torch.optim as optim
class PPOPolicy(nn.Module):
    def __init__(self, state_dim=50, action_dim=20, hidden_dim=256):
        super(PPOPolicy, self).__init__()
        self.state_dim = state_dim
        self.action_dim = action_dim
        
        # Actorç½‘ç»œ (ç­–ç•¥ç½‘ç»œ)
        self.actor = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim * 2)  # è¾“å‡ºå‡å€¼å’Œlogæ ‡å‡†å·®
        )
        
        # Criticç½‘ç»œ (ä»·å€¼ç½‘ç»œ)
        self.critic = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
        
        # åˆå§‹åŒ–å‚æ•°
        self.log_std = nn.Parameter(torch.zeros(1, action_dim))
        
    def forward(self, state):
        return self.get_action(state)
    
    def get_action(self, state, deterministic=False):
        """é‡‡æ ·åŠ¨ä½œ - ä¿®å¤è®¾å¤‡é—®é¢˜"""
        # ç¡®ä¿stateæ˜¯tensorå¹¶ä¸”åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        if not isinstance(state, torch.Tensor):
            state = torch.tensor(state, dtype=torch.float32)
        
        # å¦‚æœstateåœ¨GPUä¸Šï¼Œç¡®ä¿æ¨¡å‹ä¹Ÿåœ¨GPUä¸Š
        if state.is_cuda and next(self.parameters()).is_cuda:
            state = state.to(next(self.parameters()).device)
        elif state.is_cuda and not next(self.parameters()).is_cuda:
            state = state.cpu()
        elif not state.is_cuda and next(self.parameters()).is_cuda:
            state = state.cuda()
        
        policy_output = self.actor(state)
        mean = policy_output[:, :self.action_dim]
        std = torch.exp(self.log_std).expand_as(mean)
        
        if deterministic:
            action = mean
        else:
            normal = torch.distributions.Normal(mean, std)
            action = normal.rsample()  # é‡å‚æ•°åŒ–æŠ€å·§
        
        # ä½¿ç”¨tanhå°†åŠ¨ä½œé™åˆ¶åœ¨[-1, 1]èŒƒå›´å†…
        action = torch.tanh(action)
        
        return action
    
    def evaluate_actions(self, state, action):
        """è¯„ä¼°åŠ¨ä½œçš„æ¦‚ç‡å’Œç†µ"""
        policy_output = self.actor(state)
        mean = policy_output[:, :self.action_dim]
        std = torch.exp(self.log_std).expand_as(mean)
        
        normal = torch.distributions.Normal(mean, std)
        log_prob = normal.log_prob(action).sum(dim=-1)
        entropy = normal.entropy().sum(dim=-1)
        
        return log_prob, entropy
    
    def get_value(self, state):
        """çŠ¶æ€ä»·å€¼å‡½æ•°"""
        return self.critic(state)

class PPOAgent:
    def __init__(self, state_dim=50, action_dim=20, lr=3e-4, gamma=0.99, 
                 clip_epsilon=0.2, entropy_coef=0.01):
        # å…ˆç¡®å®šè®¾å¤‡ï¼Œå†åˆ›å»ºæ¨¡å‹
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ¤– PPOAgent ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        self.policy = PPOPolicy(state_dim, action_dim).to(self.device)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)
        
        self.gamma = gamma
        self.clip_epsilon = clip_epsilon
        self.entropy_coef = entropy_coef
        
        self.memory = []
        
    def store_transition(self, state, action, reward, next_state, done):
        """å­˜å‚¨ç»éªŒ"""
        self.memory.append({
            'state': state,
            'action': action,
            'reward': reward,
            'next_state': next_state,
            'done': done
        })
    
    def get_action(self, state, deterministic=False):
        """è·å–åŠ¨ä½œ - ä¿®å¤è®¾å¤‡é—®é¢˜"""
        # ç¡®ä¿stateæ˜¯numpyæ•°ç»„æˆ–list
        if isinstance(state, torch.Tensor):
            state = state.cpu().numpy()
        
        # åˆ›å»ºtensorå¹¶ç¡®ä¿åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        state_tensor = torch.FloatTensor(state).to(self.device)
        
        # å¦‚æœéœ€è¦batchç»´åº¦ï¼Œæ·»åŠ å®ƒ
        if state_tensor.dim() == 1:
            state_tensor = state_tensor.unsqueeze(0)  # [state_dim] -> [1, state_dim]
        
        with torch.no_grad():
            action = self.policy.get_action(state_tensor, deterministic)
        
        # è¿”å›1ç»´numpyæ•°ç»„
        return action.cpu().numpy()[0]  # ä» [1, action_dim] å˜ä¸º [action_dim]
    
    def update(self, batch_size=64, epochs=10):
        """PPOæ›´æ–°"""
        if len(self.memory) < batch_size:
            return {}
        
        # å°†æ‰€æœ‰æ•°æ®è½¬ç§»åˆ°è®¾å¤‡ä¸Š
        states = torch.FloatTensor([t['state'] for t in self.memory]).to(self.device)
        actions = torch.FloatTensor([t['action'] for t in self.memory]).to(self.device)
        rewards = torch.FloatTensor([t['reward'] for t in self.memory]).to(self.device)
        next_states = torch.FloatTensor([t['next_state'] for t in self.memory]).to(self.device)
        dones = torch.FloatTensor([t['done'] for t in self.memory]).to(self.device)
        
        # è®¡ç®—ä¼˜åŠ¿ä¼°è®¡
        with torch.no_grad():
            values = self.policy.get_value(states).squeeze()
            next_values = self.policy.get_value(next_states).squeeze()
            targets = rewards + self.gamma * next_values * (1 - dones)
            advantages = targets - values
        
        # æ ‡å‡†åŒ–ä¼˜åŠ¿
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        # PPOæ›´æ–°å¾ªç¯
        losses = []
        for _ in range(epochs):
            # éšæœºæ‰“ä¹±æ•°æ®
            indices = torch.randperm(len(states))
            
            for start in range(0, len(states), batch_size):
                end = start + batch_size
                batch_indices = indices[start:end]
                
                batch_states = states[batch_indices]
                batch_actions = actions[batch_indices]
                batch_advantages = advantages[batch_indices]
                batch_targets = targets[batch_indices]
                
                # è®¡ç®—æ–°æ—§ç­–ç•¥çš„æ¦‚ç‡
                old_log_prob, _ = self.policy.evaluate_actions(batch_states, batch_actions)
                new_log_prob, entropy = self.policy.evaluate_actions(batch_states, batch_actions)
                
                # ç­–ç•¥æ¯”ç‡
                ratio = torch.exp(new_log_prob - old_log_prob.detach())
                
                # PPOæŸå¤±
                surr1 = ratio * batch_advantages
                surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * batch_advantages
                policy_loss = -torch.min(surr1, surr2).mean()
                
                # ä»·å€¼æŸå¤±
                values_pred = self.policy.get_value(batch_states).squeeze()
                value_loss = F.mse_loss(values_pred, batch_targets)
                
                # æ€»æŸå¤±
                total_loss = policy_loss + 0.5 * value_loss - self.entropy_coef * entropy.mean()
                
                # åå‘ä¼ æ’­
                self.optimizer.zero_grad()
                total_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)
                self.optimizer.step()
                
                losses.append(total_loss.item())
        
        # æ¸…ç©ºè®°å¿†
        self.memory = []
        
        return {'total_loss': np.mean(losses) if losses else 0.0}