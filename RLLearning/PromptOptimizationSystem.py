import torch
import torch.nn as nn
import numpy as np
from typing import Dict
import utils.import_utils  # è¿™ä¼šè‡ªåŠ¨è®¾ç½®sys.path
from Prompt.Language import Language
from Prompt.MultilingualPromptGenerator import MultilingualPromptGenerator
from .PPOPolicy import PPOAgent
from .ReportEvaluator import ReportEvaluator

class PromptOptimizationSystem:
    def __init__(self):
        # å…ˆç¡®å®šè®¾å¤‡
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ”§ PromptOptimizationSystem ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # æ ¸å¿ƒç»„ä»¶
        self.prompt_generator = MultilingualPromptGenerator()
        self.rl_agent = PPOAgent(state_dim=50, action_dim=20)
        
        # è¯„ä¼°å™¨
        self.evaluator = ReportEvaluator()
        
        # è®­ç»ƒçŠ¶æ€
        self.training_mode = True
        self.episode_count = 0
        
    def generate_optimized_prompt(self, equation: str, requirements: str, 
                                language: Language = Language.CHINESE,
                                use_rl: bool = True) -> Dict:
        """ç”Ÿæˆä¼˜åŒ–åçš„prompt - ä¿®å¤è®¾å¤‡é—®é¢˜"""
        # æå–çŠ¶æ€ç‰¹å¾
        state = self._extract_state(equation, requirements)
        
        # è·å–RLåŠ¨ä½œ
        if use_rl and self.training_mode:
            try:
                rl_actions = self.rl_agent.get_action(state)
                # è½¬æ¢ä¸ºtorch tensorå¹¶ç¡®ä¿åœ¨CPUä¸Šï¼ˆå› ä¸ºpromptç”Ÿæˆå™¨å¯èƒ½åœ¨CPUä¸Šï¼‰
                rl_actions_tensor = torch.FloatTensor(rl_actions).cpu()
            except Exception as e:
                print(f"âš ï¸ RLåŠ¨ä½œç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åŠ¨ä½œ: {e}")
                rl_actions = np.random.uniform(-1, 1, 20)  # éšæœºåŠ¨ä½œä½œä¸ºfallback
                rl_actions_tensor = torch.FloatTensor(rl_actions).cpu()
        else:
            rl_actions_tensor = None
            rl_actions = None
        
        # ç”Ÿæˆprompt
        try:
            result = self.prompt_generator(
                equation, requirements, 
                rl_actions=rl_actions_tensor,
                target_language=language
            )
            
            result['state'] = state
            result['rl_actions'] = rl_actions
            
            return result
            
        except Exception as e:
            print(f"âŒ Promptç”Ÿæˆå¤±è´¥: {e}")
            # è¿”å›ä¸€ä¸ªåŸºæœ¬çš„promptä½œä¸ºfallback
            return {
                'base_prompt': f"åˆ†ææ–¹ç¨‹: {equation}\nè¦æ±‚: {requirements}",
                'optimized_prompt': f"åˆ†ææ–¹ç¨‹: {equation}\nè¦æ±‚: {requirements}",
                'equation_info': {},
                'requirements_analysis': {},
                'input_language': language,
                'output_language': language,
                'state': state,
                'rl_actions': rl_actions
            }
    
    def _extract_state(self, equation: str, requirements: str) -> np.ndarray:
        """æå–RLçŠ¶æ€ç‰¹å¾"""
        try:
            # ä½¿ç”¨EquationFeatureExtractoræå–æ–¹ç¨‹ç‰¹å¾
            equation_features = self.prompt_generator.feature_extractor.extract_features(equation).numpy()
            
            # åˆ†æéœ€æ±‚ç‰¹å¾
            lang = self.prompt_generator.language_detector.detect(requirements)
            req_analysis = self.prompt_generator.analyze_requirements(requirements, lang)
            
            # éœ€æ±‚ç‰¹å¾ç¼–ç 
            req_features = []
            req_features.append(1.0 if req_analysis.get("detail_level") == "high" else 0.0)
            req_features.append(1.0 if req_analysis.get("detail_level") == "low" else 0.0)
            req_features.append(1.0 if req_analysis.get("needs_validation", False) else 0.0)
            req_features.append(1.0 if req_analysis.get("needs_visualization", True) else 0.0)
            req_features.append(1.0 if req_analysis.get("needs_comparison", False) else 0.0)
            req_features.append(1.0 if req_analysis.get("needs_theory", False) else 0.0)
            
            # ç»„åˆç‰¹å¾
            state = np.concatenate([equation_features, req_features])
            
            # ç¡®ä¿çŠ¶æ€ç»´åº¦ä¸€è‡´
            if len(state) < 50:
                state = np.pad(state, (0, 50 - len(state)))
            else:
                state = state[:50]
                
            return state
            
        except Exception as e:
            print(f"âš ï¸ çŠ¶æ€æå–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤çŠ¶æ€: {e}")
            # è¿”å›é»˜è®¤çŠ¶æ€
            return np.random.uniform(0, 1, 50)
    
    # ... å…¶ä»–æ–¹æ³•ä¿æŒä¸å˜
    
    def evaluate_and_learn(self, equation: str, requirements: str, 
                          generated_report: str, language: Language) -> Dict:
        """è¯„ä¼°æŠ¥å‘Šå¹¶å­¦ä¹ """
        # æå–çŠ¶æ€
        state = self._extract_state(equation, requirements)
        
        # è¯„ä¼°æŠ¥å‘Šè´¨é‡
        reward, eval_details = self.evaluator.evaluate_report(
            generated_report, equation, requirements, language
        )
        
        # ç®€å•çš„ä¸‹ä¸€ä¸ªçŠ¶æ€ (è¿™é‡Œå¯ä»¥æ›´å¤æ‚)
        next_state = state  # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¯ä»¥æ ¹æ®æŠ¥å‘Šå†…å®¹æ›´æ–°çŠ¶æ€
        
        # å­˜å‚¨ç»éªŒ
        self.rl_agent.store_transition(
            state=state,
            action=self.rl_agent.get_action(state),
            reward=reward,
            next_state=next_state,
            done=True  # æ¯ä¸ªæ–¹ç¨‹æ±‚è§£æ˜¯ç‹¬ç«‹episode
        )
        
        # å®šæœŸæ›´æ–°ç­–ç•¥
        if self.episode_count % 10 == 0:  # æ¯10ä¸ªepisodeæ›´æ–°ä¸€æ¬¡
            loss_info = self.rl_agent.update()
        else:
            loss_info = {}
        
        self.episode_count += 1
        
        return {
            'reward': reward,
            'evaluation_details': eval_details,
            'training_loss': loss_info
        }
    
    def train_episode(self, equation: str, requirements: str, 
                     language: Language = Language.CHINESE) -> Dict:
        """è®­ç»ƒä¸€ä¸ªepisode"""
        # ç”Ÿæˆä¼˜åŒ–åçš„prompt
        prompt_result = self.generate_optimized_prompt(
            equation, requirements, language, use_rl=True
        )
        
        # è¿™é‡Œåº”è¯¥è°ƒç”¨DeepSeek APIç”ŸæˆæŠ¥å‘Š
        # ä¸ºæ¼”ç¤ºç›®çš„ï¼Œæˆ‘ä»¬æ¨¡æ‹Ÿä¸€ä¸ªæŠ¥å‘Š
        simulated_report = self._simulate_report_generation(prompt_result['optimized_prompt'])
        
        # è¯„ä¼°å’Œå­¦ä¹ 
        learning_result = self.evaluate_and_learn(
            equation, requirements, simulated_report, language
        )
        
        return {
            'prompt_result': prompt_result,
            'learning_result': learning_result,
            'generated_report': simulated_report
        }
    
    def _simulate_report_generation(self, prompt: str) -> str:
        """æ¨¡æ‹ŸæŠ¥å‘Šç”Ÿæˆ (å®é™…åº”è¯¥è°ƒç”¨DeepSeek API)"""
        # è¿™é‡Œåº”è¯¥è°ƒç”¨çœŸå®çš„DeepSeek API
        # ä¸ºæ¼”ç¤ºç›®çš„ï¼Œè¿”å›æ¨¡æ‹ŸæŠ¥å‘Š
        return f"""
        åŸºäºpromptç”Ÿæˆçš„æ¨¡æ‹ŸæŠ¥å‘Š:
        
        {prompt[:100]}...
        
        è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿçš„æŠ¥å‘Šå†…å®¹ï¼ŒåŒ…å«ï¼š
        1. æ–¹ç¨‹åˆ†æ
        2. æ•°å€¼æ–¹æ³•é€‰æ‹©
        3. Pythonä»£ç å®ç°
        4. ç»“æœå¯è§†åŒ–
        5. è¯¯å·®åˆ†æ
        
        åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥æ˜¯DeepSeekç”Ÿæˆçš„çœŸå®æŠ¥å‘Šã€‚
        """
    
    def save_model(self, path: str):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'policy_state_dict': self.rl_agent.policy.state_dict(),
            'optimizer_state_dict': self.rl_agent.optimizer.state_dict(),
            'episode_count': self.episode_count
        }, path)
    
    def load_model(self, path: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(path)
        self.rl_agent.policy.load_state_dict(checkpoint['policy_state_dict'])
        self.rl_agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.episode_count = checkpoint['episode_count']